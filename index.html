<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="cvpr, workshop, computer vision, physics simulation, computer graphics, visual learning, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon.png">

  <title>SimVision @CVPR25</title>
  <meta name="description" content="Vision Meets Physics, CVPR 2025 Workshop">

  <!--Open Graph Related Stuff
  <meta property="og:title" content="Vision Meets Physics Workshop"/>
  <meta property="og:url" content="https://visionmeetphysics.github.io"/>
  <meta property="og:description" content="Vision Meets Physics, CVPR 2025 Workshop"/>
  <meta property="og:site_name" content="Vision Meets Physics Workshop"/>
  <meta property="og:image" content="https://visionmeetphysics.github.io/CVPR2025/static/img/site/teaser.jpg"/>
  -->

  <!--Twitter Card Stuff
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Language for 3D Scenes Workshop"/>
  <meta name="twitter:image" content="https://languagefor3dscenes.github.io/ICCV2023/static/img/site/teaser.jpg">
  <meta name="twitter:url" content="https://languagefor3dscenes.github.io/ICCV2023"/>
  <meta name="twitter:description" content="Language for 3D Scenes, ECCV 2022 Workshop"/>
  -->
  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 125px;
      max-height: 125px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>1st Workshop on Vision Meets Physics:</h1></center>
    <center><h1>Synergizing Physical Simulation and Computer Vision</h1></center>
    <center><h2>CVPR 2025 Workshop</h2></center>
    <center>Room 104 A - June 12 (full day), 2025</center>
  </div>
</div>

<hr />


<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/watch?v=gyJDGrbLknI">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br>

<div class="alert alert-info" role="alert">
  <b>Join Zoom Meeting  <a href="https://kaust.zoom.us/j/95818223470">here</a>.</b>
</div>
 -->


<div class="row" id="teaser">  
    <div style="text-align: center;">  
    <img src="static/img/site/teaser.png" style="width: 80%; height: auto;
		transform: scaleX(1.2);"/>
  </div>
</div>


<!-- 
<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/teaser.png">
</div>
<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/b.png">
</div>
 -->






<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
    As the intersection of computer vision and physics continues to evolve, two competing perspectives have emerged on how best to simulate and model the world. One belief holds that accurate simulations must be rooted in pure physics and 3D dynamics, while another asserts that data-driven approaches, such as training video foundation models, can fully capture and represent the world. This workshop aims to bring together researchers from both schools of thought to foster discussion and collaboration. We encourage researchers to explore both perspectives and seek common ground on how these methods can enhance one another.
    </p>
    <p>
      The fusion of physical simulation and computer vision holds tremendous potential across a range of applications, from scientific research and generative AI to robotics, gaming, and extended realities (XR). By combining the strengths of both fields, we can push the boundaries of realistic content creation, advancing technologies that require high-fidelity simulations—whether for training models, designing immersive environments, or automating complex tasks.
    </p>
    <p>
      Through presentations, discussions, and collaborative sessions, this workshop will explore a variety of cutting-edge topics. Our goal is to foster an interdisciplinary dialogue and drive the development of next-generation technologies that blend the strengths of both physics-based and data-driven approaches. We hope to contribute to the creation of more robust and versatile simulation-ready assets and systems that seamlessly bridge the gap between the virtual and physical worlds. 
    </p>
  </div>
</div>

<!--<p><br /></p>

-->
<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule (Nashville Central Daylight Time)</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>08:45-09:00</td>
          <td>Welcome</td>
          <td></td>
        </tr>
        <tr>
          <td>09:00-09:45</td>
          <td><b>Wei-Chiu Ma</b> (Cornell University)</td>
          <td>TBA</td>
        </tr>
        <tr>
          <td>09:45-10:30</td>
          <td><b>Tsung-Yi Lin</b> (NVIDIA Research)</td>
          <td>TBA</td>
        </tr>

        <tr>
          <td>10:30-11:15</td>
          <td>Student Lightning Talks</td>
            <td>
             • <b>Tianyi Xie</b> (UCLA) - Towards Physics-Informed Content Generation<br/>
             • <b>Michelle Guo</b> (Stanford) - TBA<br/>
             • <b>Navami Kairanda</b> (MPI) - Unfolding Cloth: Neural Deformation Fields for Simulation and Monocular Tracking
           </td>
        </tr>

        <tr>
          <td>11:15-12:00</td>
          <td><b>Yin Yang</b> (University of Utah)</td>
          <td>A New Paradigm for Parallel Simulation</td>
        </tr>
        
        <tr>
          <td>12:00-13:00</td>
          <td>Lunch Break</td>
          <td></td>
        </tr>

        <tr>
          <td>13:00-13:45</td>
          <td><b>Katerina Fragkiadaki</b> (Carnegie Mellon University)</td>
          <td>From Explicit Physics Engines to Neural Simulators with Generative Models</td>
        </tr>
        
        <tr>
          <td>13:45-14:30</td>
          <td><b>Jiajun Wu</b> (Stanford University)</td>
          <td>TBA</td>
        </tr>
        
        <tr>
          <td>14:30-14:45</td>
          <td>Coffee Break</td>
          <td></td>
        </tr>

        <tr>
          <td>14:45-15:30</td>
          <td>Student Lightning Talks</td>
          <td>
             • <b>Yue Chang</b> (UToronto) - Fast Physics through Eigenanalysis over the Shape Space<br/>
             • <b>Hanxiao Jiang</b> (UIUC) - PhysTwin: Physics-Informed Modelling from Videos<br/>
             • <b>Yifei Li</b> (MIT) - TBA
           </td>
        </tr>

        <tr>
          <td>15:30-16:15</td>
          <td><b>Ming Lin</b> (University of Maryland)</td>
          <td>TBA</td>
        </tr>

        <tr>
          <td>16:20-17:20</td>
          <td>Panel Discussion</td>
          <td></td>
        </tr>

        <tr>
          <td>17:20-17:30</td>
          <td>Concluding Remarks</td>
          <td></td>
        </tr>

      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.cornell.edu/~weichiu/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/weichiu-mit-min.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.cornell.edu/~weichiu/">Wei-Chiu Ma</a></b> is an Assistant Professor of Computer Science at Cornell University. His research lies at the intersection of 3D/4D computer vision and robotics. He is interested in building AI systems that can understand, reconstruct, and re-simulate the dynamic world, and leverage these capabilities to enable more robust autonomous systems or advance entertainment applications. Previously, he was a Senior Research Scientist at Uber ATG R&D and Waabi working on self-driving vehicles.
    </p>
  </div>
</div>
<p><br/></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://tsungyilin.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/tsungyi.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://tsungyilin.info/">Tsung-Yi Lin</a></b> is a principal research scientist at NVIDIA Research. He was previously at Google Research, Brain Team. He works on computer vision and machine learning. He did his PhD at Cornell University and Cornell Tech, where he was advised by Serge Belongie. He did his masters at University California, San Diego and his bachelors at National Taiwan University. He received the Best Student Paper Award for Focal Loss at ICCV 2017. He led the creation of the COCO dataset which received the PAMI Mark Everingham Prize at ICCV 2023 and Koenderink Prize at ECCV 2024.
    </p>
  </div>
</div>
<p><br/></p> 
        
<div class="row">
  <div class="col-md-2">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/Jiajun_Wu.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is an Assistant Professor of Computer Science and, by courtesy, of Psychology at Stanford University, working on computer vision, machine learning, and computational cognitive science. Before joining Stanford, he was a Visiting Faculty Researcher at Google Research. He received his PhD in Electrical Engineering and Computer Science from the Massachusetts Institute of Technology. Wu's research has been recognized through the Young Investigator Programs (YIP) by ONR and by AFOSR, the NSF CAREER award, the Okawa research grant, paper awards and finalists at ICCV, CVPR, SIGGRAPH Asia, CoRL, and IROS, dissertation awards from ACM, AAAI, and MIT, the 2020 Samsung AI Researcher of the Year, and faculty research awards from J.P. Morgan, Samsung, Amazon, and Meta.
    </p>
  </div>
</div>
<p><br/></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.cmu.edu/~katef/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/KaterinaFragkiadaki.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b> is the JPMorgan Chase Associate Professor in the Machine Learning Department at Carnegie Mellon University. She received her undergraduate degree in Electrical and Computer Engineering from the National Technical University of Athens, and her Ph.D. from the University of Pennsylvania. She subsequently held postdoctoral positions at UC Berkeley and Google Research. Her research focuses on enabling few-shot and continual learning for perception, action, and language grounding. Her work has been recognizedby the Best Ph.D. Thesis Award, NSF CAREER Award, AFOSR and DARPA Young Investigator Awards, as well as faculty research awards from Google, Toyota Research Institute, Amazon, NVIDIA, UPMC, and Sony. She served as a Program Chair for ICLR 2024.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://yangzzzy.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/YinYang.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://yangzzzy.github.io/">Yin Yang</a></b> is an associate professor with Kahlert School of Computing, University of Utah. He co-directs Utah Graphics Lab with his colleague Cem Yuksel. He is also affiliated with Utah Robotic Center. Before joining the University of Utah (the birthplace of Computer Graphics), he was a faculty member at University of New Mexico and Clemson University. He received his Ph.D. from University of Texas at Dallas (with David Daniel fellowship). He is a recipient of the NSF CRII award (2015) and CAREER award (2019). His research aims to develop efficient and customized computing methods for challenging problems in Graphics, Simulation, Deep Learning, Vision, Robotics, and many other applied areas.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.umd.edu/~lin/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/Ming_Lin_UMIACS.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.umd.edu/~lin/">Ming C. Lin</a></b> is a Distinguished University Professor at University of Maryland at College Park. She is also an Amazon Scholar, working on Virtual Try-On. Her research interests include Machine Learning, Differentiable Physics & Differentiable Programming, Physically-based Modeling, Simulation and Animation. She has received several honors and awards, including IEEE VGTC VR Technical Achievement Award, Washington Academy of Sciences Distinguished Career Award and several best paper awards. She is a Fellow of National Academy of Inventors, ACM, IEEE, Eurographics, ACM SIGGRAPH Academy, and IEEE VR Academy. She is an elected Board of Director (BOD) of Computing Research Association (CRA), as well as a member of the CRA-WP Board of Directors and EC of AsiaGraphics (AG).
    </p>
  </div>
</div>
<p><br /></p>

</div>

<!-- 
      
<p><br /></p>
<div class="row">
  <div class="col-md-2">
    <a href="https://mingyuliu.net/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/mingyu_2023.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://mingyuliu.net/">Ming-Yu Liu</a></b> is a Vice President of Research at NVIDIA and a Fellow of IEEE. He leads the Deep Imagination Research group at NVIDIA, which currently focuses on Generative AI for Physical AI. His research team has helped create several new product categories for NVIDIA, including NVIDIA Cosmos, a developer-first world foundation model platform for Physical AI, NVIDIA Edify, a family of Generative AI models that powers Getty Images and Shutterstock's GenAI services, NVIDIA Canvas [GauGAN], a real-time painting tool that uses GANs to turn simple brushstrokes into photorealistc images, and NVIDIA Maxine [LivePortrait], an AI-first cloud-native video streaming platform. His research group constantly publishes scientific papers in top-tier AI conferences regularly, including NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV, and SIGGRAPH. Several of their papers received prestigious awards.
    </p>
  </div>
</div>
<p><br /></p>
      
<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Panelists</h2>
  </div>
</div> -->

<!-- <div class="row">
  <div class="col-md-2">
    <a href="https://www.cc.gatech.edu/~dbatra/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/batra.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a></b> is an Associate Professor in the School of
      Interactive Computing at Georgia Tech and a Research Scientist at Facebook AI Research (FAIR).
      His research interests lie at the intersection of machine learning, computer vision, natural language processing,
      and AI. The long-term goal of his research is to develop agents that 'see' (or more generally
      perceive their environment through vision, audition, or other senses), 'talk' (i.e. hold a natural language dialog
      grounded in their environment), 'act' (e.g. navigate their environment and interact with it to accomplish goals),
      and 'reason' (i.e., consider the long-term consequences of their actions).
      He is a recipient of the Presidential Early Career Award for Scientists and Engineers (PECASE) 2019.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.cmu.edu/~katef/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/fragk.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b> is an Assistant Professor in the Machine Learning Department at Carnegie Mellon.
      Prior to joining MLD's faculty she worked as a postdoctoral researcher first at UC Berkeley working with Jitendra Malik and then
      at Google Research in Mountain View working with the video group. Katerina is interested in building machines that understand the
      stories that videos portray, and, inversely, in using videos to teach machines about the world. The penultimate goal is
      to build a machine that understands movie plots, and the ultimate goal is to build a machine that would want to watch Bergman over this.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.utexas.edu/~mooney/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/mooney_raymond.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.utexas.edu/~mooney/">Raymond Mooney</a></b> is a Professor in the Department of Computer Science at the University of Texas at Austin. He received his Ph.D. in 1988 from the University of Illinois at Urbana/Champaign. He is an author of over 160 published research papers, primarily in the areas of machine learning and natural language processing. He was the President of the International Machine Learning Society from 2008-2011, program co-chair for AAAI 2006, general chair for HLT-EMNLP 2005, and co-chair for ICML 1990. He is a Fellow of the American Association for Artificial Intelligence, the Association for Computing Machinery, and the Association for Computational Linguistics and the recipient of best paper awards from AAAI-96, KDD-04, ICML-05 and ACL-07.
    </p>
  </div>
</div>
<p><br /></p> -->

<p><br /></p>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://weify627.github.io/">
      <img class="people-pic" src="static/img/people/fangyinw.jpg" />
    </a>
    <div class="people-name">
      <a href="https://weify627.github.io/">Fangyin Wei</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="">
      <img class="people-pic" src="static/img/people/xiang_donglai_2051017.jpg" />
    </a>
    <div class="people-name">
      <a href="https://xiangdonglai.github.io/">Donglai Xiang</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="">
      <img class="people-pic" src="static/img/people/qma.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://qianlim.github.io/">Qianli Ma</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="">
      <img class="people-pic" src="static/img/people/YifeiLi.jpg" />
    </a>
    <div class="people-name">
      <a href="https://people.csail.mit.edu/liyifei/">Yifei Li</a>
      <h6>MIT</h6>
    </div>
  </div>

</div>
<p><br /></p>
<p><br /></p>

<div class="row" id="advisors">
  <div class="col-xs-12">
    <h2>Senior Advisors</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://www.cs.umd.edu/~lin/">
      <img class="people-pic" src="static/img/people/Ming_Lin_UMIACS.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.cs.umd.edu/~lin/">Ming Lin</a>
      <h6>University of Maryland</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.math.ucla.edu/~cffjiang/">
      <img class="people-pic" src="static/img/people/ChenfanfuJiang.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.math.ucla.edu/~cffjiang/">Chenfanfu Jiang</a>
      <h6>University of California, Los Angeles</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://shenlong.web.illinois.edu/">
      <img class="people-pic" src="static/img/people/ShenlongWang.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://shenlong.web.illinois.edu/">Shenlong Wang</a>
      <h6>University of Illinois Urbana-Champaign</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://research.nvidia.com/labs/prl/author/david-i.w.-levin/">
      <img class="people-pic" src="static/img/people/DavidLevin.jpg" />
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/labs/prl/author/david-i.w.-levin/">David Levin</a>
      <h6>NVIDIA / </h6>
      <h6>University of Toronto</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://tsungyilin.info/">
      <img class="people-pic" src="static/img/people/tsungyi.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://tsungyilin.info/">Tsung-Yi Lin</a>
      <h6>NVIDIA</h6>
    </div>
  </div>
</div>

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers, please use <b>visionmeetphysics@gmail.com</b>.
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://languagefor3dscenes.github.io/">languagefor3dscenes</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
